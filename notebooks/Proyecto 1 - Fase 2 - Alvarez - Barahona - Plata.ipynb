{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65486335",
   "metadata": {},
   "source": [
    "### María Sofía Álvarez - Brenda Barahona - Álvaro Plata\n",
    "<h1 align='center'>Proyecto 1: Analítica de textos - Preprocesamiento</h1>\n",
    "\n",
    "En esta fase del proyecto, nos encargaremos de construir las pipelines necesarias para desplegar nuestros modelos en una API. Para ello, únicamente nos remitiremos a construir los 3 modelos que nos dieron los mejores resultados en la fase 1 de este proyecto, después de realizar el ajuste de hiperparámetros desarrollado en la fase anterior. Este notebook se divide en ciertas fases. Primero, se realiza todo el preprocesamiento, que es común a todos los modelos. Segundo, se terminan de construir las pipelines propias de cada modelo, de acuerdo con los hiperparámetros que fueron ajustados en la fase 1 (solo con ellos, para ahorrar tiempo de cómputo). Por último, se exportan los tres modelos.\n",
    "\n",
    "Si desea ver a fondo alguna parte del perfilamiento, preprocesamiento de datos realizado, o ajuste de hiperparámetrod, remítase al repositorio de github de la fase 1 de este proyecto, disponible en <a href=\"https://github.com/sofiaalvarezlopez/Proyecto-1-BI\">este link</a>.\n",
    "\n",
    "## Importación de librerías\n",
    "Importamos las librerías necesarias para el desarrollo de este proyecto. Las librerías instaladas son exactamente las mismas que las que se instalaron para la fase 1 de este proyecto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "aee7fd68",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ESAI\n",
    "import re\n",
    "import nltk\n",
    "import keras\n",
    "import spacy\n",
    "import inflect\n",
    "import sent2vec # Para descargar esta libreria, es necesario descargarla desde GitHub https://github.com/epfml/sent2vec\n",
    "import stopwords\n",
    "import numpy as np\n",
    "import unicodedata\n",
    "import pandas as pd\n",
    "import contractions\n",
    "import seaborn as sns\n",
    "#nltk.download('wordnet')\n",
    "#nltk.download('omw-1.4')\n",
    "import pandas_profiling as pp\n",
    "from joblib import dump, load\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from tensorflow.keras.models import Sequential\n",
    "from sklearn.utils import resample, class_weight\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from nltk.stem import SnowballStemmer, WordNetLemmatizer\n",
    "from sklearn.metrics import precision_score, make_scorer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from clases import Preprocessing, StemAndLemmatize, VectorizeLSTM, LSTMBuilder\n",
    "from keras.layers import LSTM, Dense, Embedding, TextVectorization, Input, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "283a6d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a37bd96",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8e4e61",
   "metadata": {},
   "source": [
    "Procedemos, entonces, a ver los datos suministrados. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76c40049",
   "metadata": {},
   "outputs": [],
   "source": [
    "diagnoses =pd.read_csv('ApoyoDiagnosticoEstudiante/medical_text_clasificacion.csv')\n",
    "X, Y = diagnoses.drop(['problems_described'], axis=1), diagnoses['problems_described']\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y,stratify=Y,test_size=0.95, random_state=28)\n",
    "data_train = pd.concat( [X_train, Y_train], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b10797",
   "metadata": {},
   "source": [
    "## Preprocesamiento\n",
    "Iniciamos con la fase de preprocesamiento de los datos:\n",
    "### Extracción de entidades médicas\n",
    "De acuerdo con [4], para clasificación en contextos médicos es útil extraer entidades médicas para la clasificación, como se hizo en el proyecto pasado. No obstante, se obtuvieron mejores resultados con las entidades en todos los casos, así que se omitirá este pedazo:\n",
    "```python\n",
    "nlp = en_ner_bionlp13cg_md.load()\n",
    "def medical_entities(text):\n",
    "    entities = []\n",
    "    doc = nlp(text)\n",
    "    for ent in doc.ents:\n",
    "        entities.append(ent.text)\n",
    "    return ' '.join(entities)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7000e9a9",
   "metadata": {},
   "source": [
    "### Manejo de Ruido \n",
    "En esta sección se quitará o modificará todo lo que se considere como ruido:\n",
    "\n",
    "+ Caracteres no ascii: Hace parte importante del preprocesamiento de las palabras. Con caracteres no-ascii, el preprocesamiento puede verse terriblemente perjudicado.\n",
    "+ Se pasará de mayusculas a minusculas: Asimismo, es importante que todas las palabras tengan una capitalización homogénea (en este caso, queremos que estén en minúscula).\n",
    "+ Se eliminará la puntuación: Por otro lado, consideramos que la puntuación no provee información adicional en este contexto. Adicionalmente, de no eliminarse, puede aumentar la dimensionalidad de los datos sin proveer más información. Por ejemplo, no tiene sentido pensar que \"almuerzo!\" y \"almuerzo\" sean palabras diferentes. Por ello removemos toda la puntuación usando expresiones regulares.\n",
    "+ Se reemplazarán los números: Ahora, podemos suponer que los números no proveen información relevante para el problema en cuestión. Estos pueden también agregar dimensionalidad inutilmente al problema.\n",
    "+ Se quitarán las fechas (si las hay) también: las fechas son irrelevantes para el contexto del problema.\n",
    "+ Se quitarán las palabras vacias (artículos, pronombres, preposiciones): Estas se denominan stop-words, en inglés. Son palabras que se usan en muchos contextos (como 'the') y no aportan información significativa en la construcción del modelo. Asimismo, definimos nuestras propias stopwords de acuerdo con el perfilamiento realizado, pues son palabras que no aportan significativamente al contexto."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bfbfe53",
   "metadata": {},
   "source": [
    "```python\n",
    "def remove_non_ascii(words):\n",
    "    \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def to_lowercase(words):\n",
    "    \"\"\"Convert all characters to lowercase from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = word.lower()\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "    \n",
    "\n",
    "def remove_punctuation(words):\n",
    "    \"\"\"Remove punctuation from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = re.sub(r'[^\\w\\s]', '', word)\n",
    "        if new_word != '':\n",
    "            new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def remove_numbers(words):\n",
    "    p = inflect.engine()\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = re.sub('\\d+.*', '', word)\n",
    "        if not word.isnumeric() and new_word != '':\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "def remove_dates(words):\n",
    "    \"\"\"Replace all dates in our data\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = re.sub(r'\\d+/\\d+/\\d+', '', word)\n",
    "        if new_word != '':\n",
    "            new_words.append(new_word)\n",
    "    return new_words\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd4f7dc",
   "metadata": {},
   "source": [
    "De todas estas palabras, encontramos que solamente tumor y lesion pueden ser relevantes para nuestro análisis. Por lo tanto, las quitamos todas, excepto estas dos. Además, en un análisis preliminar encontramos otras palabras irrelevantes, las cuales también eliminamos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dfbd103",
   "metadata": {},
   "source": [
    "```python\n",
    "#En una primera iteracion nos dimos cuenta que las palabras \"paty\", \"patients\" aparece frecuentemente en todas las enfermedades,\n",
    "#estos serán eliminados por que no agregan información valiosa. \n",
    "our_stopwords = [\"paty\",\"patients\",\"p\",\"study\",\"result\", \"human\", \"humans\", \"monkey\", \"monkeys\", \n",
    "                 \"diseases\", \"studied\",\"first\", \"rat\", \"patient\", \"case\", \"p less\", \"treatment\", \n",
    "                 \"group\", \"associated\", \"result\", \"may\", \"effect\", \"compared\", \"use\", \"cases\", \"year\", \n",
    "                 \"years\", \"age\", \"study\", \"disease\", \"found\", \"normal\", \"month\", \"although\", \"per cent\",\n",
    "                 \"one\", \"two\", \"three\", \"four\", \"n\", \"children\", \"women\"]\n",
    "\n",
    "def remove_stopwords(words):\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word not in stopwords.words('english') and word not in our_stopwords:\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "```\n",
    "Asimismo, tenemos la función de eliminación del ruido global:\n",
    "\n",
    "``` python\n",
    "def noise_elimination(words):\n",
    "    words = to_lowercase(words)\n",
    "    words = remove_non_ascii(words)\n",
    "    words = remove_numbers(words)\n",
    "    words = remove_dates(words)\n",
    "    words = remove_punctuation(words)\n",
    "    words = remove_stopwords(words)\n",
    "    return words\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa899a0e",
   "metadata": {},
   "source": [
    "Con esto, ya tenemos casi listo nuestro proceso de eliminación del ruido. Primero, llamamos a la función <code>fix</code> de la librería ```contraction``` para aquellas contracciones que no están separadas en dos palabras. Esta elimina todas las ocurrencias de contracciones en inglés, reemplazándolas por su equivalente sin contracción. Una vez realizado este paso, \"tokenizamos\" las historias clínicas. Para poder evaluar cada palabra por separado y aplicar los pasos de preprocesamiento, hacemos la tokenización en palabras individuales usando el módulo ```word_tokenize```. Finalmente, aplicamos la función```noise_elimination``` definida previamente.\n",
    "\n",
    "Más adelante, lo que realmente nos servirá será volver a tener los documentos sin tokenización para el proceso de vectorización (sea tf-idf, o BioSentVec, como se vera mas adelante). Entonces, volvemos a juntar todas las palabras para cada documento y retornamos eso. También retornamos las palabras tokenizadas con el fin de realizar la lematización estemización más adelante.\n",
    "\n",
    "Note que estas funciones las aplicamos tanto sobre los medical abstracts iniciales, como las palabras clave que obtuvimos con la librería de SpaCy descrita previamente.\n",
    "\n",
    "```python\n",
    "def preprocessing(X):\n",
    "    new_X_train= X.apply(contractions.fix) #Aplica la corrección de las contracciones\n",
    "    new_X_train = new_X_train.apply(word_tokenize)\n",
    "    new_X_train = new_X_train.apply(noise_elimination) #Aplica la eliminación del ruido\n",
    "    X_train = new_X_train.apply(lambda x: ' '.join(map(str, x)))\n",
    "    return new_X_train, X_train\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8450b0",
   "metadata": {},
   "source": [
    "### Normalización: Stemming y Lemmatization \n",
    "Aplicaremos tecnicas como \"Stemming\" y \"Lemmatization\" sobre la columna \"medical_abstracts\" y \"medical_entities\". \n",
    "\n",
    "En esta parte del preprocesamiento, hacemos una eliminación de prefijos y sufijos, así como una lematización de los verbos. En el caso del Stemming hay varios algoritmos que podemos utilizar: Porter, Snowball (Porter2) o Lancaster (Paice-Husk). De acuerdo a lo que encontramos,  <a href=\"https://stackoverflow.com/questions/10554052/what-are-the-major-differences-and-benefits-of-porter-and-lancaster-stemming-alg\"> la agresividad en el corte de raíces de las palabras de estos algoritmos aumenta, siendo Porter el menos agresivo y Lancaster el más agresivo </a>. En este sentido, parece ser que Lancaster (a pesar de ser el más eficiente de todos), puede ser poco riguroso y así crear muchas ambigüedades. Asimismo, Porter2 es un poco más agresivo que Porter, sin perder mucho el origen de las palabras y con un tiempo de cómputo razonable. El mismo Porter, creador del algoritmo, argumenta que es una mejora de su algoritmo original. Con el fin de tener la mejor preparación de las palabras, en un tiempo de cómputo razonable, decidimos usar Porter2. En el caso de la lematización, sí usamos WordNetLemmatizer() al ser el más usado en el mundo del procesamiento de textos.\n",
    "```python\n",
    "#Funciones de \"Stemming\" y \"Lemmatization\"\n",
    "def stem_words(words):\n",
    "    \"\"\"Stem words in list of tokenized words\"\"\"\n",
    "    stemmer = SnowballStemmer('english')\n",
    "    stems = []\n",
    "    for word in words:\n",
    "        stem = stemmer.stem(word)\n",
    "        stems.append(stem)\n",
    "    return stems\n",
    "\n",
    "def lemmatize_verbs(words):\n",
    "    \"\"\"Lemmatize verbs in list of tokenized words\"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmas = []\n",
    "    for word in words:\n",
    "        lemma = lemmatizer.lemmatize(word, pos='v')\n",
    "        lemmas.append(lemma)\n",
    "    return lemmas\n",
    "\n",
    "def stem_and_lemmatize(words):\n",
    "    stems = stem_words(words)\n",
    "    lemmas = lemmatize_verbs(words)\n",
    "    return stems + lemmas\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bbb6ca9",
   "metadata": {},
   "source": [
    "Y creamos la clase correspondiente para las funciones:\n",
    "``` python\n",
    "class StemAndLemmatize():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def transform(self,X,y=None):\n",
    "        stems_abs = stem_words(X[\"medical_abstracts\"])\n",
    "        lemmas_abs = lemmatize_verbs(X[\"medical_abstracts\"])\n",
    "        stems_ents = stem_words(X[\"medical_entities\"])\n",
    "        lemmas_ents = lemmatize_verbs(X[\"medical_entities\"])\n",
    "        return X\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c74f0b",
   "metadata": {},
   "source": [
    "Por último, es posible que, tras el preprocesamiento, algunos datos hayan quedado con entidades médicas nulas. Entonces, es necesario eliminar estas filas. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5759a7d",
   "metadata": {},
   "source": [
    "Finalmente, con todas las funciones definidas, creamos el pipeline de preprocesamiento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad9e2385",
   "metadata": {},
   "outputs": [],
   "source": [
    "preproc = [\n",
    "    (\"preprocessing\", Preprocessing()),\n",
    "    (\"stem_lemmatize\", StemAndLemmatize())\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "45eea7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe  = Pipeline(preproc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "68008d97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>medical_abstracts</th>\n",
       "      <th>problems_described</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5157</th>\n",
       "      <td>[biliari, gut, function, follow, shock, aim, c...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4803</th>\n",
       "      <td>[inpati, theophyllin, toxic, prevent, factor, ...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9804</th>\n",
       "      <td>[transor, approach, manag, intradur, lesion, c...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7183</th>\n",
       "      <td>[control, trial, communiti, base, coronari, re...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9719</th>\n",
       "      <td>[high, preval, antibodi, hepat, c, virus, hepa...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      medical_abstracts  problems_described\n",
       "5157  [biliari, gut, function, follow, shock, aim, c...                   5\n",
       "4803  [inpati, theophyllin, toxic, prevent, factor, ...                   5\n",
       "9804  [transor, approach, manag, intradur, lesion, c...                   3\n",
       "7183  [control, trial, communiti, base, coronari, re...                   4\n",
       "9719  [high, preval, antibodi, hepat, c, virus, hepa...                   1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datos_train_procesados = pipe.fit_transform(data_train, data_train['problems_described'])\n",
    "datos_train_procesados.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137bfd99",
   "metadata": {},
   "source": [
    "Veamos una muestra de los textos preprocesados:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c509b0",
   "metadata": {},
   "source": [
    "Finalmente, almacenamos los datos de entrenamiento en un archivo <code>.csv</code> denominado: <code>proyecto1_fase2_datos_entrenamiento.csv</code>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e075174a",
   "metadata": {},
   "outputs": [],
   "source": [
    "datos_train_procesados.to_csv(\"proyecto1_fase2_datos_preprocesados.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ece05ab",
   "metadata": {},
   "source": [
    "## Modelo: LSTM\n",
    "En la iteración pasada, diseñamos tres algoritmos de Machine Learning para probar: Naïve-Bayes, OneVsRest y una red neuronal usando una LSTM (Long-Short Term Memory). De estos, el que mejores métricas arrojó fue el LSTM. Por lo tanto, debido a que buscamos optimizar la precisión de nuestro modelo para el cliente, este es el que desplegaremos en la API. Si desea ver más información acerca de la LSTM, remítase al repositorio del proyecto anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a0c73d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "diagnoses =pd.read_csv('ApoyoDiagnosticoEstudiante/medical_text_clasificacion.csv')\n",
    "X, Y = diagnoses.drop(['problems_described'], axis=1), diagnoses['problems_described']\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y,stratify=Y,test_size=0.95, random_state=28)\n",
    "data_train = pd.concat( [X_train, Y_train], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16fbef43",
   "metadata": {},
   "source": [
    "### Manejo de desbalanceo de las clases\n",
    "\n",
    "Ahora, uno de los mayores problemas de la clasificación es el contexto desbalanceado. Una opción sería reducir el conjunto de datos hasta que todas las clases queden con un número de abstracts igual al tamaño de la clase de menor cantidad de abstracts. No obstante, por lo general la idea es no reducir el conjunto de datos. Otra opción, como en los algoritmos anteriores, sería usar SMOTE. No obstante, esto es computacionalmente muy costoso para la red.\n",
    "\n",
    "Lo que sí podemos hacer es considerar pesos. Así, el modelo podrá prestar mayor atención a las clases minoritarias. Para ello, usaremos la librería de <code>sk-learn</code> y lo pasaremos como un objeto al modelo que construiremos más adelante:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1b8e215b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = class_weight.compute_class_weight(\n",
    "                class_weight = 'balanced',\n",
    "                classes = np.unique(diagnoses['problems_described']), \n",
    "                y = diagnoses['problems_described'])\n",
    "train_class_weights_ = dict(enumerate(class_weights, start=1))\n",
    "train_class_weights = dict(enumerate(class_weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9640f129",
   "metadata": {},
   "source": [
    "Podemos ver los pesos asociados a cada una de las clases, en orden ascendente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "428e1e35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 0.9128946367440092,\n",
       " 2: 1.932367149758454,\n",
       " 3: 1.5,\n",
       " 4: 0.9463722397476341,\n",
       " 5: 0.6010518407212622}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_class_weights_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889b607b",
   "metadata": {},
   "source": [
    "### Modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac0b2f3",
   "metadata": {},
   "source": [
    "Usando el modelo de vectorización de BioSentVec, se obtuvo, utilizando la búsqueda de hiperparámetros, que el mejor modelo era aquel que no tenía ninguna capa adicional a las consideradas inicialmente (dos capas LSTM con sus respectivas capas de dropout), un dropout de 0.1 (i.e. una tasa de pérdida bastante pequeña) y cada una de las capas con 64 neuronas.\n",
    "\n",
    "Finalmente, se tiene una capa softmax con 5 neuronas, que corresponden con las 5 clases del problema. Es importante mencionar que la variable categórica Y se debe convertir a la funcionalidad de Keras para el correcto funcionamiento de la red."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "32e67bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = data_train.drop(['problems_described'], axis=1), data_train['problems_described']\n",
    "Y = keras.utils.np_utils.to_categorical(Y)[:,1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c05f23e8",
   "metadata": {},
   "source": [
    "Y, por último, creamos la vectorización:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5220a913",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.steps.append(('vectorizer', VectorizeLSTM()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf2efa8",
   "metadata": {},
   "source": [
    "Veamos qué tiene la pipeline hasta ahora:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "66dbcae2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('preprocessing',\n",
       "                 <clases.Preprocessing object at 0x7f7e68e48670>),\n",
       "                ('stem_lemmatize',\n",
       "                 <clases.StemAndLemmatize object at 0x7f7e68e481f0>),\n",
       "                ('vectorizer',\n",
       "                 <clases.VectorizeLSTM object at 0x7f7e68e4ac40>)])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c379290",
   "metadata": {},
   "source": [
    "Finalmente, creamos un clasificador de Keras usando el modelo:\n",
    "```python\n",
    "class LSTMBuilder():\n",
    "    def __call__(self):\n",
    "        output=5\n",
    "        model = Sequential(name=\"LSTM\")\n",
    "        # Agregamos una capa LSTM con el tamanio de entrada de los embedded abstracts y 64 neuronas en la capa\n",
    "        model.add(LSTM(units=64, return_sequences=True, \n",
    "                    input_shape=(1, 700)))\n",
    "        model.add(Dropout(0.1))\n",
    "        # Agregamos una segunda capa LSTM con 16 neuronas\n",
    "        model.add(LSTM(units=64, return_sequences=False))\n",
    "        # Con su respectiva capa de dropout\n",
    "        model.add(Dropout(0.1))\n",
    "        # Definimos la capa de salida\n",
    "        model.add(Dense(output, activation='softmax'))\n",
    "        # Compilo\n",
    "        model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=[keras.metrics.Precision(name='precision')])\n",
    "        return model\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b208f0bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = KerasClassifier(LSTMBuilder(), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a86574ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.steps.append((('model', clf)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "33163a08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model successfuly loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-10 23:22:53.851758: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "15/15 [==============================] - 3s 58ms/step - loss: 1.6324 - precision: 0.0000e+00 - val_loss: 1.6063 - val_precision: 0.0000e+00\n",
      "Epoch 2/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 1.5974 - precision: 0.0000e+00 - val_loss: 1.5732 - val_precision: 0.0000e+00\n",
      "Epoch 3/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 1.4874 - precision: 0.0000e+00 - val_loss: 1.4706 - val_precision: 0.0000e+00\n",
      "Epoch 4/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 1.2607 - precision: 0.7286 - val_loss: 1.2905 - val_precision: 0.6667\n",
      "Epoch 5/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 1.0310 - precision: 0.7130 - val_loss: 1.2113 - val_precision: 0.6562\n",
      "Epoch 6/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.8752 - precision: 0.6962 - val_loss: 1.1051 - val_precision: 0.6364\n",
      "Epoch 7/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.7627 - precision: 0.7364 - val_loss: 1.0797 - val_precision: 0.6173\n",
      "Epoch 8/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.6891 - precision: 0.7514 - val_loss: 1.0698 - val_precision: 0.5833\n",
      "Epoch 9/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.6309 - precision: 0.7646 - val_loss: 1.0576 - val_precision: 0.5843\n",
      "Epoch 10/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.5834 - precision: 0.7716 - val_loss: 1.0662 - val_precision: 0.6484\n",
      "Epoch 11/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.5488 - precision: 0.7870 - val_loss: 1.0393 - val_precision: 0.6000\n",
      "Epoch 12/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.5058 - precision: 0.8009 - val_loss: 1.0478 - val_precision: 0.5980\n",
      "Epoch 13/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.4560 - precision: 0.8242 - val_loss: 1.0963 - val_precision: 0.6289\n",
      "Epoch 14/100\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.4030 - precision: 0.8455 - val_loss: 1.0775 - val_precision: 0.5865\n",
      "Epoch 15/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.3723 - precision: 0.8736 - val_loss: 1.1311 - val_precision: 0.6061\n",
      "Epoch 16/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.3346 - precision: 0.8728 - val_loss: 1.0465 - val_precision: 0.6091\n",
      "Epoch 17/100\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.3061 - precision: 0.8942 - val_loss: 1.1777 - val_precision: 0.5810\n",
      "Epoch 18/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.2848 - precision: 0.8968 - val_loss: 1.1845 - val_precision: 0.6000\n",
      "Epoch 19/100\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.2473 - precision: 0.9056 - val_loss: 1.1421 - val_precision: 0.6147\n",
      "Epoch 20/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.2184 - precision: 0.9242 - val_loss: 1.2266 - val_precision: 0.5943\n",
      "Epoch 21/100\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.2045 - precision: 0.9302 - val_loss: 1.1893 - val_precision: 0.6126\n",
      "Epoch 22/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.1940 - precision: 0.9408 - val_loss: 1.3294 - val_precision: 0.5688\n",
      "Epoch 23/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.1825 - precision: 0.9308 - val_loss: 1.2812 - val_precision: 0.5888\n",
      "Epoch 24/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.1564 - precision: 0.9601 - val_loss: 1.2539 - val_precision: 0.5862\n",
      "Epoch 25/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.1441 - precision: 0.9580 - val_loss: 1.3123 - val_precision: 0.5641\n",
      "Epoch 26/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.1352 - precision: 0.9561 - val_loss: 1.3372 - val_precision: 0.5982\n",
      "Epoch 27/100\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.1316 - precision: 0.9558 - val_loss: 1.4028 - val_precision: 0.5804\n",
      "Epoch 28/100\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.1232 - precision: 0.9621 - val_loss: 1.3558 - val_precision: 0.5877\n",
      "Epoch 29/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.1183 - precision: 0.9686 - val_loss: 1.4238 - val_precision: 0.5603\n",
      "Epoch 30/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.1077 - precision: 0.9646 - val_loss: 1.3905 - val_precision: 0.5948\n",
      "Epoch 31/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.0978 - precision: 0.9686 - val_loss: 1.4589 - val_precision: 0.5983\n",
      "Epoch 32/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.0915 - precision: 0.9686 - val_loss: 1.4733 - val_precision: 0.5929\n",
      "Epoch 33/100\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0808 - precision: 0.9708 - val_loss: 1.5276 - val_precision: 0.5776\n",
      "Epoch 34/100\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.0816 - precision: 0.9688 - val_loss: 1.5359 - val_precision: 0.5965\n",
      "Epoch 35/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.0759 - precision: 0.9770 - val_loss: 1.6178 - val_precision: 0.5726\n",
      "Epoch 36/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.0744 - precision: 0.9708 - val_loss: 1.6330 - val_precision: 0.5789\n",
      "Epoch 37/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.0611 - precision: 0.9792 - val_loss: 1.6382 - val_precision: 0.5652\n",
      "Epoch 38/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.0649 - precision: 0.9729 - val_loss: 1.6407 - val_precision: 0.5714\n",
      "Epoch 39/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.0602 - precision: 0.9771 - val_loss: 1.7800 - val_precision: 0.5664\n",
      "Epoch 40/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.0564 - precision: 0.9833 - val_loss: 1.7427 - val_precision: 0.5641\n",
      "Epoch 41/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.0528 - precision: 0.9770 - val_loss: 1.7032 - val_precision: 0.6034\n",
      "Epoch 42/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.0608 - precision: 0.9729 - val_loss: 1.7739 - val_precision: 0.5877\n",
      "Epoch 43/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.0591 - precision: 0.9750 - val_loss: 1.7583 - val_precision: 0.5776\n",
      "Epoch 44/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.0559 - precision: 0.9812 - val_loss: 1.9096 - val_precision: 0.5625\n",
      "Epoch 45/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.0474 - precision: 0.9833 - val_loss: 1.7883 - val_precision: 0.5690\n",
      "Epoch 46/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.0400 - precision: 0.9874 - val_loss: 1.8475 - val_precision: 0.6000\n",
      "Epoch 47/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.0460 - precision: 0.9833 - val_loss: 1.9109 - val_precision: 0.5932\n",
      "Epoch 48/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.0469 - precision: 0.9833 - val_loss: 1.8658 - val_precision: 0.5862\n",
      "Epoch 49/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.0401 - precision: 0.9874 - val_loss: 1.9122 - val_precision: 0.5897\n",
      "Epoch 50/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.0387 - precision: 0.9895 - val_loss: 1.9010 - val_precision: 0.6018\n",
      "Epoch 51/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.0474 - precision: 0.9875 - val_loss: 1.9686 - val_precision: 0.5739\n",
      "Epoch 52/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.0437 - precision: 0.9896 - val_loss: 2.0510 - val_precision: 0.5214\n",
      "Epoch 53/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.0364 - precision: 0.9854 - val_loss: 1.9265 - val_precision: 0.5726\n",
      "Epoch 54/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.0449 - precision: 0.9833 - val_loss: 1.9850 - val_precision: 0.5965\n",
      "Epoch 55/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.0299 - precision: 0.9916 - val_loss: 2.0222 - val_precision: 0.5776\n",
      "Epoch 56/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.0360 - precision: 0.9874 - val_loss: 1.9985 - val_precision: 0.5826\n",
      "Epoch 57/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.0288 - precision: 0.9916 - val_loss: 2.0464 - val_precision: 0.5826\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.0265 - precision: 0.9916 - val_loss: 2.0386 - val_precision: 0.5690\n",
      "Epoch 59/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.0359 - precision: 0.9916 - val_loss: 2.0005 - val_precision: 0.5913\n",
      "Epoch 60/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.0294 - precision: 0.9896 - val_loss: 2.1105 - val_precision: 0.5702\n",
      "Epoch 61/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.0313 - precision: 0.9895 - val_loss: 2.1287 - val_precision: 0.5664\n",
      "Epoch 62/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.0281 - precision: 0.9875 - val_loss: 2.1308 - val_precision: 0.5690\n",
      "Epoch 63/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.0325 - precision: 0.9875 - val_loss: 2.1025 - val_precision: 0.5812\n",
      "Epoch 64/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.0374 - precision: 0.9854 - val_loss: 2.0908 - val_precision: 0.5565\n",
      "Epoch 65/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.0321 - precision: 0.9875 - val_loss: 2.2469 - val_precision: 0.5000\n",
      "Epoch 66/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.0253 - precision: 0.9917 - val_loss: 2.1291 - val_precision: 0.5603\n",
      "Epoch 67/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.0313 - precision: 0.9875 - val_loss: 2.1863 - val_precision: 0.5565\n",
      "Epoch 68/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.0258 - precision: 0.9917 - val_loss: 2.2274 - val_precision: 0.5391\n",
      "Epoch 69/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.0327 - precision: 0.9896 - val_loss: 2.2046 - val_precision: 0.5652\n",
      "Epoch 70/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.0312 - precision: 0.9896 - val_loss: 2.2205 - val_precision: 0.5593\n",
      "Epoch 71/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.0267 - precision: 0.9896 - val_loss: 2.1680 - val_precision: 0.5897\n",
      "Epoch 72/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.0223 - precision: 0.9917 - val_loss: 2.2632 - val_precision: 0.5556\n",
      "Epoch 73/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.0283 - precision: 0.9916 - val_loss: 2.2254 - val_precision: 0.5690\n",
      "Epoch 74/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.0271 - precision: 0.9875 - val_loss: 2.2347 - val_precision: 0.5752\n",
      "Epoch 75/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.0240 - precision: 0.9917 - val_loss: 2.2169 - val_precision: 0.5517\n",
      "Epoch 76/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.0257 - precision: 0.9916 - val_loss: 2.3431 - val_precision: 0.5299\n",
      "Epoch 77/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.0275 - precision: 0.9854 - val_loss: 2.2076 - val_precision: 0.5776\n",
      "Epoch 78/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.0274 - precision: 0.9896 - val_loss: 2.2089 - val_precision: 0.5789\n",
      "Epoch 79/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.0232 - precision: 0.9875 - val_loss: 2.2713 - val_precision: 0.5812\n",
      "Epoch 80/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.0270 - precision: 0.9896 - val_loss: 2.2176 - val_precision: 0.5776\n",
      "Epoch 81/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.0185 - precision: 0.9937 - val_loss: 2.3088 - val_precision: 0.5526\n",
      "Epoch 82/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.0216 - precision: 0.9896 - val_loss: 2.2687 - val_precision: 0.5690\n",
      "Epoch 83/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.0224 - precision: 0.9917 - val_loss: 2.2697 - val_precision: 0.5752\n",
      "Epoch 84/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.0192 - precision: 0.9917 - val_loss: 2.3112 - val_precision: 0.5652\n",
      "Epoch 85/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.0266 - precision: 0.9896 - val_loss: 2.3146 - val_precision: 0.5727\n",
      "Epoch 86/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.0248 - precision: 0.9854 - val_loss: 2.3246 - val_precision: 0.5526\n",
      "Epoch 87/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.0294 - precision: 0.9854 - val_loss: 2.3171 - val_precision: 0.5714\n",
      "Epoch 88/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.0290 - precision: 0.9896 - val_loss: 2.3056 - val_precision: 0.5726\n",
      "Epoch 89/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.0220 - precision: 0.9896 - val_loss: 2.2856 - val_precision: 0.5678\n",
      "Epoch 90/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.0199 - precision: 0.9896 - val_loss: 2.2776 - val_precision: 0.5690\n",
      "Epoch 91/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.0205 - precision: 0.9917 - val_loss: 2.3245 - val_precision: 0.5929\n",
      "Epoch 92/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.0152 - precision: 0.9979 - val_loss: 2.3655 - val_precision: 0.5841\n",
      "Epoch 93/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.0237 - precision: 0.9896 - val_loss: 2.3614 - val_precision: 0.5565\n",
      "Epoch 94/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.0236 - precision: 0.9917 - val_loss: 2.4386 - val_precision: 0.5172\n",
      "Epoch 95/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.0149 - precision: 0.9937 - val_loss: 2.3118 - val_precision: 0.5690\n",
      "Epoch 96/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.0180 - precision: 0.9917 - val_loss: 2.3576 - val_precision: 0.5664\n",
      "Epoch 97/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.0255 - precision: 0.9896 - val_loss: 2.4463 - val_precision: 0.5495\n",
      "Epoch 98/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.0212 - precision: 0.9896 - val_loss: 2.3974 - val_precision: 0.5556\n",
      "Epoch 99/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 0.0295 - precision: 0.9875 - val_loss: 2.4419 - val_precision: 0.5043\n",
      "Epoch 100/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 0.0221 - precision: 0.9896 - val_loss: 2.3411 - val_precision: 0.5752\n"
     ]
    }
   ],
   "source": [
    "pipe_elegida = pipe.fit(X, Y, \n",
    "                        model__class_weight=train_class_weights,\n",
    "                        model__validation_split=0.2, \n",
    "                        model__epochs= 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2294c1c",
   "metadata": {},
   "source": [
    "### Prueba de la calidad del modelo\n",
    "Veamos la matriz de confusión y otras métricas pertinentes para este modelo. Para un análisis más exhaustivo, remítase al anterior proyecto."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8bf6b2",
   "metadata": {},
   "source": [
    "### Obtención de las estadísticas\n",
    "Veamos la probabilidad de que un texto determinado pertenezca a cada clase:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "45a21d82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model successfuly loaded\n"
     ]
    }
   ],
   "source": [
    "pred = pipe_elegida.predict_proba(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b9e6fc",
   "metadata": {},
   "source": [
    "### Exportación del modelo\n",
    "Una vez entrenado el modelo, procedemos a guardarlo y exportarlo. Debido a que no existe una manera directa de almacenar un modelo de keras dentro de una pipeline de scikit learn, es necesario guardarlos por separado. Así, guardamos primero el modelo de Keras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "95e1091b",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'save'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [34]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mpipe_elegida\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnamed_steps\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./assets/keras_model.h5\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      2\u001b[0m pipe_elegida\u001b[38;5;241m.\u001b[39mnamed_steps[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m      3\u001b[0m dump(pipe_elegida, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./assets/modelo.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'save'"
     ]
    }
   ],
   "source": [
    "pipe_elegida.named_steps['model'].model.save('./assets/keras_model.h5')\n",
    "pipe_elegida.named_steps['model'].model = None\n",
    "dump(pipe_elegida, './assets/modelo.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "335e9617",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5.0689712e-05, 4.0981346e-03, 1.7193420e-03, 2.3285644e-04,\n",
       "       9.9389899e-01], dtype=float32)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ae6961ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>medical_abstracts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5157</th>\n",
       "      <td>biliari gut function follow shock aim charact ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4803</th>\n",
       "      <td>inpati theophyllin toxic prevent factor object...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9804</th>\n",
       "      <td>transor approach manag intradur lesion craniov...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7183</th>\n",
       "      <td>control trial communiti base coronari rehabili...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9719</th>\n",
       "      <td>high preval antibodi hepat c virus hepatocellu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5017</th>\n",
       "      <td>essenti hyperten sign search concept cardin im...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9129</th>\n",
       "      <td>acut toler morphin analgesia continu infus sin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6539</th>\n",
       "      <td>hepat lesion rabbit induc acoust cavit tissu d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>714</th>\n",
       "      <td>flumazenil neonat side benzodiazepin pregnanc ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1202</th>\n",
       "      <td>coronari trap complement activ product c3a des...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>600 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      medical_abstracts\n",
       "5157  biliari gut function follow shock aim charact ...\n",
       "4803  inpati theophyllin toxic prevent factor object...\n",
       "9804  transor approach manag intradur lesion craniov...\n",
       "7183  control trial communiti base coronari rehabili...\n",
       "9719  high preval antibodi hepat c virus hepatocellu...\n",
       "...                                                 ...\n",
       "5017  essenti hyperten sign search concept cardin im...\n",
       "9129  acut toler morphin analgesia continu infus sin...\n",
       "6539  hepat lesion rabbit induc acoust cavit tissu d...\n",
       "714   flumazenil neonat side benzodiazepin pregnanc ...\n",
       "1202  coronari trap complement activ product c3a des...\n",
       "\n",
       "[600 rows x 1 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d2d0c1",
   "metadata": {},
   "source": [
    "<h2 id='bibliografia'>Bibliografía</h2>\n",
    "\n",
    "---\n",
    "\n",
    "<a id='geron'>[1]</a> Géron, A. (2017). Hands-on machine learning with Scikit-Learn and TensorFlow : concepts, tools, and techniques to build intelligent systems. Sebastopol, CA: O'Reilly Media. ISBN: 978-1491962299\n",
    "\n",
    "<a id='nlp_profiler'>[2]</a> https://towardsdatascience.com/nlp-profiler-profiling-datasets-with-one-or-more-text-columns-9b791193db89\n",
    "\n",
    "[3] https://www.kaggle.com/code/neomatrix369/nlp-profiler-simple-dataset/notebook\n",
    "\n",
    "[4] Clinical Text Classification: https://www.kaggle.com/ritheshsreenivasan/clinical-text-classification"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
